{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import yaml\n",
    "import math\n",
    "from collections import Counter\n",
    "sys.path.append('.')\n",
    "sys.path.append('./..')\n",
    "import model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'PanjivaRecordID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_attr_with_id(row, attr, val2id_dict):\n",
    "    val = row[attr]\n",
    "    if val not in val2id_dict.keys():\n",
    "        print(attr, val)\n",
    "        return None\n",
    "    else:\n",
    "        return val2id_dict[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(\n",
    "        df,\n",
    "        save_dir\n",
    "):\n",
    "    global id_col\n",
    "\n",
    "    feature_columns = list(df.columns)\n",
    "    feature_columns.remove(id_col)\n",
    "\n",
    "    dict_DomainDims = {}\n",
    "    col_val2id_dict = {}\n",
    "\n",
    "    for col in sorted(feature_columns):\n",
    "        vals = list(set(df[col]))\n",
    "\n",
    "        # ----\n",
    "        #\n",
    "        #   0 : item1 ,\n",
    "        #   1 : item2 ,\n",
    "        #   ...\n",
    "        # ----\n",
    "        id2val_dict = {\n",
    "            e[0]: e[1]\n",
    "            for e in enumerate(vals, 0)\n",
    "        }\n",
    "\n",
    "        # ----\n",
    "        #\n",
    "        #   item1 : 0 ,\n",
    "        #   item2 : 0 ,\n",
    "        #   ...\n",
    "        # ----\n",
    "        val2id_dict = {\n",
    "            v: k for k, v in id2val_dict.items()\n",
    "        }\n",
    "        col_val2id_dict[col] = val2id_dict\n",
    "\n",
    "        # Replace\n",
    "        df[col] = df.apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "        dict_DomainDims[col] = len(id2val_dict)\n",
    "\n",
    "    print(' Feature columns :: ', feature_columns)\n",
    "    print('dict_DomainDims ', dict_DomainDims)\n",
    "\n",
    "    # -------------\n",
    "    # Save the domain dimensions\n",
    "    # -------------\n",
    "\n",
    "    file = 'domain_dims.pkl'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            dict_DomainDims,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    return df, col_val2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSCode_cleanup(list_df):\n",
    "    hscode_col = 'HSCode'\n",
    "\n",
    "    # ----- #\n",
    "    # Expert curated HS codes\n",
    "    hs_code_filter_file =  'hscode_filter_file.txt'\n",
    "\n",
    "    tmp = pd.read_csv(\n",
    "        hs_code_filter_file,\n",
    "        index_col=None,\n",
    "        header=None\n",
    "    )\n",
    "    target_codes = list(tmp[0])\n",
    "\n",
    "    def hsc_proc(_code):\n",
    "        return str(_code)[:4]\n",
    "\n",
    "    target_codes = list(sorted([hsc_proc(_) for _ in target_codes]))\n",
    "\n",
    "    def filter_by_ExpertHSCodeList(_code, target_codes):\n",
    "        if _code[:2] in target_codes or _code[:4] in target_codes:\n",
    "            return _code\n",
    "        return None\n",
    "    def add_preceeding_zero(_code):\n",
    "        _code = _code.strip()\n",
    "        if len(_code) > 6:\n",
    "            _code = _code[:6]\n",
    "        elif len(_code) == 5:\n",
    "            _code = '0' + _code\n",
    "        return _code\n",
    "\n",
    "    list_processed_df = []\n",
    "    for df in list_df:\n",
    "        df = df.dropna()\n",
    "        df[hscode_col] = df[hscode_col].astype(str)\n",
    "        \n",
    "        df[hscode_col] = df[hscode_col].apply(add_preceeding_zero)\n",
    "\n",
    "        df[hscode_col] = df[hscode_col].apply(\n",
    "            filter_by_ExpertHSCodeList,\n",
    "            args=(target_codes,)\n",
    "        )\n",
    "        df = df.dropna()\n",
    "        list_processed_df.append(df)\n",
    "    # --------- #\n",
    "\n",
    "    return list_processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_frequency_values(df):\n",
    "    global id_col\n",
    "    freq_bound = 3\n",
    "\n",
    "    freq_column_value_filters = {}\n",
    "\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    # ----\n",
    "    # figure out which entities are to be removed\n",
    "    # ----\n",
    "    for c in feature_cols:\n",
    "        values = list(df[c])\n",
    "        freq_column_value_filters[c] = []\n",
    "\n",
    "        obj_counter = Counter(values)\n",
    "        for _item, _count in obj_counter.items():\n",
    "            if _count < freq_bound:\n",
    "                freq_column_value_filters[c].append(_item)\n",
    "    print('Removing :: ')\n",
    "    for c, _items in freq_column_value_filters.items():\n",
    "        print('column : ', c, 'count', len(_items))\n",
    "\n",
    "    print(' DF length : ', len(df))\n",
    "    for col, val in freq_column_value_filters.items():\n",
    "        df = df.loc[\n",
    "            (~df[col].isin(val))\n",
    "        ]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train_data():\n",
    "    \n",
    "    use_cols = ['PanjivaRecordID', 'ShipperPanjivaID','ShipmentDestination', 'TransportMethod', 'HSCode']\n",
    "    \n",
    "    files = ['./../../Data/panjiva_costa_rica_exports_02_2017.csv']\n",
    "    list_df = [ pd.read_csv(_file, usecols = use_cols, low_memory=False) for _file in files]\n",
    "    list_df = [ df.dropna() for df in list_df ]\n",
    "    \n",
    "    # use_cols\n",
    "    print (list_df[0].columns)\n",
    "   \n",
    "\n",
    "    list_df = HSCode_cleanup(list_df)\n",
    "    \n",
    "    master_df = None\n",
    "    for df in list_df:\n",
    "        if master_df is None:\n",
    "            master_df = pd.DataFrame(df, copy=True)\n",
    "        else:\n",
    "            master_df = master_df.append(\n",
    "                df,\n",
    "                ignore_index=True\n",
    "            )\n",
    "\n",
    "    master_df = remove_low_frequency_values(master_df)\n",
    "\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PanjivaRecordID', 'ShipperPanjivaID', 'ShipmentDestination',\n",
      "       'TransportMethod', 'HSCode'],\n",
      "      dtype='object')\n",
      "Removing :: \n",
      "column :  ShipperPanjivaID count 51\n",
      "column :  ShipmentDestination count 9\n",
      "column :  TransportMethod count 0\n",
      "column :  HSCode count 26\n",
      " DF length :  2686\n"
     ]
    }
   ],
   "source": [
    "train_df = clean_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PanjivaRecordID</th>\n",
       "      <th>ShipperPanjivaID</th>\n",
       "      <th>ShipmentDestination</th>\n",
       "      <th>TransportMethod</th>\n",
       "      <th>HSCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4432892</td>\n",
       "      <td>29142268</td>\n",
       "      <td>United States</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>950669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4449340</td>\n",
       "      <td>29142268</td>\n",
       "      <td>United States</td>\n",
       "      <td>Air</td>\n",
       "      <td>950669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4356348</td>\n",
       "      <td>45735618</td>\n",
       "      <td>India</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4356412</td>\n",
       "      <td>36856429</td>\n",
       "      <td>India</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4361788</td>\n",
       "      <td>41666887</td>\n",
       "      <td>Chile</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>940340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>4377660</td>\n",
       "      <td>44479072</td>\n",
       "      <td>India</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>4381116</td>\n",
       "      <td>45735618</td>\n",
       "      <td>India</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>4387068</td>\n",
       "      <td>39846525</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>4393212</td>\n",
       "      <td>29142268</td>\n",
       "      <td>United States</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>950669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>4406780</td>\n",
       "      <td>45722841</td>\n",
       "      <td>India</td>\n",
       "      <td>Maritime</td>\n",
       "      <td>440796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PanjivaRecordID  ShipperPanjivaID ShipmentDestination TransportMethod  \\\n",
       "18           4432892          29142268       United States        Maritime   \n",
       "42           4449340          29142268       United States             Air   \n",
       "86           4356348          45735618               India        Maritime   \n",
       "87           4356412          36856429               India        Maritime   \n",
       "88           4361788          41666887               Chile        Maritime   \n",
       "139          4377660          44479072               India        Maritime   \n",
       "152          4381116          45735618               India        Maritime   \n",
       "158          4387068          39846525          Bangladesh        Maritime   \n",
       "172          4393212          29142268       United States        Maritime   \n",
       "174          4406780          45722841               India        Maritime   \n",
       "\n",
       "     HSCode  \n",
       "18   950669  \n",
       "42   950669  \n",
       "86   440799  \n",
       "87   440349  \n",
       "88   940340  \n",
       "139  440729  \n",
       "152  440349  \n",
       "158  440399  \n",
       "172  950669  \n",
       "174  440796  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir = '.'\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_valuesId_dict_file = 'column_valuesId_dict.pkl'\n",
    "column_valuesId_dict_path = os.path.join(save_dir, column_valuesId_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature columns ::  ['ShipperPanjivaID', 'ShipmentDestination', 'TransportMethod', 'HSCode']\n",
      "dict_DomainDims  {'HSCode': 57, 'ShipmentDestination': 30, 'ShipperPanjivaID': 91, 'TransportMethod': 3}\n"
     ]
    }
   ],
   "source": [
    "train_df, col_val2id_dict = convert_to_ids(\n",
    "    train_df,\n",
    "    save_dir\n",
    ")\n",
    "id_col = 'PanjivaRecordID'\n",
    "\n",
    "feature_cols = list(train_df.columns)\n",
    "feature_cols.remove(id_col)\n",
    "feature_cols = list(sorted(feature_cols))\n",
    "all_cols = [id_col]\n",
    "all_cols.extend(feature_cols)\n",
    "\n",
    "train_df = train_df[all_cols]\n",
    "train_df_file='train_data.csv'\n",
    "train_df.to_csv(train_df_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "'''\n",
    "Create co-occurrence between entities using training data. \n",
    "Returns a dict { Domain1_+_Domain2 : __matrix__ }\n",
    "Domain1 and Domain2 are sorted lexicographically\n",
    "'''\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)   \n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col 1 & 2 HSCode ShipmentDestination (57, 30) >> (57, 30)\n",
      "Col 1 & 2 HSCode ShipperPanjivaID (57, 91) >> (57, 91)\n",
      "Col 1 & 2 HSCode TransportMethod (57, 3) >> (57, 3)\n",
      "Col 1 & 2 ShipmentDestination ShipperPanjivaID (30, 91) >> (30, 91)\n",
      "Col 1 & 2 ShipmentDestination TransportMethod (30, 3) >> (30, 3)\n",
      "Col 1 & 2 ShipperPanjivaID TransportMethod (91, 3) >> (91, 3)\n"
     ]
    }
   ],
   "source": [
    "coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coOccMatrix_dict.pkl\",'wb') as fh:\n",
    "    pickle.dump( coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"domain_dims.pkl\",'rb') as fh:\n",
    "    domain_dims = pickle.load( fh )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSCode_+_ShipmentDestination 318.0\n",
      "HSCode_+_ShipperPanjivaID 176.0\n",
      "HSCode_+_TransportMethod 342.0\n",
      "ShipmentDestination_+_ShipperPanjivaID 253.0\n",
      "ShipmentDestination_+_TransportMethod 595.0\n",
      "ShipperPanjivaID_+_TransportMethod 253.0\n"
     ]
    }
   ],
   "source": [
    "X_ij_max = []\n",
    "for k,v in coOccMatrix_dict.items():\n",
    "    print(k, np.max(v))\n",
    "    X_ij_max.append(np.max(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[318.0, 176.0, 342.0, 253.0, 595.0, 253.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model_file.get_model(\n",
    "    domain_dimesnsions = list(domain_dims.values()),\n",
    "    num_domains = 4,\n",
    "    embed_dim = 16,\n",
    "    _X_ij_max = X_ij_max\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "feature_cols = sorted(list(train_df.columns))\n",
    "feature_cols = list(feature_cols)\n",
    "feature_cols.remove(id_col)\n",
    "data = train_df[feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = len(feature_cols)\n",
    "num_c = nd *(nd-1) // 2\n",
    "X_ij = np.zeros([data.shape[0], num_c ])\n",
    "\n",
    "k = 0\n",
    "for i in range(len(feature_cols)):\n",
    "    for j in range(i+1, len(feature_cols)):\n",
    "        key = feature_cols[i]+ '_+_' + feature_cols[j]\n",
    "        print('>>',key)\n",
    "        \n",
    "        for d in range(data.shape[0]):\n",
    "            e1 = data[d][i]\n",
    "            e2 = data[d][j]\n",
    "            X_ij[d][k] = coOccMatrix_dict[key][e1][e2]\n",
    "        k+=1\n",
    "        \n",
    "        \n",
    "with open(\"X_ij.pkl\",\"wb\") as fh:\n",
    "    pickle.dump(X_ij,fh,pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> HSCode_+_ShipmentDestination\n",
      ">> HSCode_+_ShipperPanjivaID\n",
      ">> HSCode_+_TransportMethod\n",
      ">> ShipmentDestination_+_ShipperPanjivaID\n",
      ">> ShipmentDestination_+_TransportMethod\n",
      ">> ShipperPanjivaID_+_TransportMethod\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "split_layer (Lambda)            [(None, 1), (None, 1 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 16)        912         split_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 16)        480         split_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 16)        1456        split_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 16)        48          split_layer[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_4 (Dot)                     (None, 1, 1)         0           embedding_3[0][0]                \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_5 (Dot)                     (None, 1, 1)         0           embedding_3[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_6 (Dot)                     (None, 1, 1)         0           embedding_5[0][0]                \n",
      "                                                                 embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 1)         57          split_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 1)         30          split_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 1)            0           dot_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 1)         91          split_layer[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 1)            0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 1)         3           split_layer[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1)            0           dot_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 1)            0           dot_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 1)            0           dot_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 1)         0           reshape_1[0][0]                  \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 1)         0           reshape_3[0][0]                  \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 1)         0           reshape_5[0][0]                  \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1, 1)         0           reshape_7[0][0]                  \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 1, 1)         0           reshape_9[0][0]                  \n",
      "                                                                 embedding_4[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 1, 1)         0           reshape_11[0][0]                 \n",
      "                                                                 embedding_6[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1)            0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1)            0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1)            0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 1)            0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 1)            0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stack_layer (Lambda)            (None, 6, 1)         0           reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "                                                                 reshape_8[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,077\n",
      "Trainable params: 3,077\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "data[0].shape = [2] does not start with indices[0].shape = [3]\n\t [[{{node training/Adam/gradients/loss/stack_layer_loss/Sum_grad/DynamicStitch}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0775df315a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_ij\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/Code/WWF/skunk02/skunk02/src/model_1/model_file.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y_true)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m     model.train(\n",
      "\u001b[0;32m~/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/skunk02/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/skunk02/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: data[0].shape = [2] does not start with indices[0].shape = [3]\n\t [[{{node training/Adam/gradients/loss/stack_layer_loss/Sum_grad/DynamicStitch}}]]"
     ]
    }
   ],
   "source": [
    "model_file.train_model(\n",
    "    model,\n",
    "    data,\n",
    "    X_ij\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
