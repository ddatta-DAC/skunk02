{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import Model, Sequential, layers\n",
    "from keras.layers import Lambda\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Embedding, Dot, Reshape, Add, Average, Concatenate, TimeDistributed, Bidirectional,LSTM, Multiply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_domains = 4 \n",
    "\n",
    "domain_dims = [10,20,25,30]\n",
    "input_emb_dim = 256\n",
    "lstm_dim = 32\n",
    "ctx_dim_1 = 32\n",
    "interaction_dim = 32\n",
    "num_neg_samples = 10\n",
    "domain_emb_wt = [\n",
    "    np.random.random([domain_dims[i],input_emb_dim])\n",
    "    for i in range(num_domains)\n",
    "]\n",
    "\n",
    "RUN_MODE = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    num_domains,\n",
    "    domain_dims,\n",
    "    domain_emb_wt,\n",
    "    lstm_dim,\n",
    "    interaction_layer_dim,\n",
    "    context_dim,\n",
    "    num_neg_samples = 10,\n",
    "    RUN_MODE = 'train'    \n",
    "    ):\n",
    "    \n",
    "    ctx_dim_1 = context_dim\n",
    "    interaction_dim = interaction_layer_dim\n",
    "    input_emb_dim = domain_emb_wt[0].shape[-1]\n",
    "    n_timesteps = num_domains + 2\n",
    "    \n",
    "    \n",
    "    def tf_stack(x):\n",
    "        import tensorflow as tf\n",
    "        import keras.backend as K\n",
    "\n",
    "        x1 = tf.stack(\n",
    "                x,\n",
    "                axis = 1\n",
    "        )\n",
    "        return x1\n",
    "\n",
    "\n",
    "    def tf_squeeze(x):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        x1 = tf.squeeze(\n",
    "                x,\n",
    "                axis = 1\n",
    "        )\n",
    "        return x1\n",
    "\n",
    "    def tf_split_dplus2(x):\n",
    "        global n_timesteps\n",
    "        return tf.split(\n",
    "                x,\n",
    "                num_or_size_splits=n_timesteps,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "    def tf_reduce_sum(x):\n",
    "        import tensorflow as tf\n",
    "        return tf.math.reduce_sum(\n",
    "            x,\n",
    "            keepdims=False,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    def split_squeeze_numDomains(x):\n",
    "        import tensorflow as tf\n",
    "        import keras.backend as K\n",
    "        global num_domains\n",
    "        x1 = tf.split(\n",
    "                x,\n",
    "                num_or_size_splits = num_domains,\n",
    "                axis = 1\n",
    "        )\n",
    "        x2 = [tf.squeeze(_x2,axis=1) for _x2 in x1]\n",
    "        return x2\n",
    "\n",
    "    def tf_reduce_mean_kdims_axis1(x):\n",
    "        import tensorflow as tf\n",
    "        return tf.math.reduce_mean(\n",
    "                x,\n",
    "                axis=1,\n",
    "                keepdims=True )\n",
    "\n",
    "    def tf_split_squeeze_axis1(x):\n",
    "        import tensorflow as tf\n",
    "        x1 = tf.split(\n",
    "                x,\n",
    "                num_or_size_splits = x.shape[1],\n",
    "                axis = 1\n",
    "        )\n",
    "        x2 = [tf.squeeze(_x2,axis=1) for _x2 in x1]\n",
    "        return x2\n",
    "\n",
    "    def tf_reciprocal(x):\n",
    "        import tensorflow as tf\n",
    "        epsilon = .000001\n",
    "        return tf.math.reciprocal(\n",
    "             x + epsilon\n",
    "        )\n",
    "\n",
    "    def tf_sigmoid(x):\n",
    "        import tensorflow as tf\n",
    "        return tf.sigmoid(\n",
    "             x \n",
    "        )\n",
    "    \n",
    "            \n",
    "    # ================= Define the weights ===================== #\n",
    "\n",
    "    BD_LSTM_layer = Bidirectional(\n",
    "        LSTM(\n",
    "            units=lstm_dim, \n",
    "            return_sequences=True\n",
    "        ), \n",
    "        input_shape = (n_timesteps, input_emb_dim), \n",
    "        merge_mode = None\n",
    "    )\n",
    "\n",
    "    # Embedding layer for each domain\n",
    "    list_Entity_Embed = [\n",
    "        Embedding(\n",
    "            input_dim = domain_dims[i], \n",
    "            output_dim = input_emb_dim, \n",
    "            embeddings_initializer =  keras.initializers.Constant(value = domain_emb_wt[i]),\n",
    "            name = 'entity_embedding_'+str(i)\n",
    "        ) for i in range(num_domains)\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Dense layer for getting the Context vectors\n",
    "    list_FNN_1 = [ Dense( ctx_dim_1 ,activation = 'relu', use_bias=True) for i in range(1,n_timesteps-1) ]\n",
    "    list_FNN_2 = [ Dense( interaction_dim ,activation = 'relu') for i in range(1,n_timesteps-1) ]\n",
    "    # Dense layer for transforming the input vectors\n",
    "    xform_Inp_FNN = [Dense( interaction_dim ,activation = None,use_bias=True) for i in range(num_domains)]\n",
    "\n",
    "    # ========================================================= #\n",
    "\n",
    "    def process(input_indices, _type = 'pos'):\n",
    "        # Split the inputs\n",
    "        split_input = Lambda(tf_split_squeeze_axis1)(input_indices)   \n",
    "        split_emb = []\n",
    "        for i in range(num_domains):\n",
    "            split_emb.append(list_Entity_Embed[i](split_input[i]))\n",
    "\n",
    "        split_emb = [Lambda(tf_squeeze)(_) for _ in split_emb]\n",
    "        # input embedding now has shape [ ?, num_domains, 256]\n",
    "        input_emb = Lambda(tf_stack)(split_emb)\n",
    "\n",
    "        mean_layer_op = Lambda(tf_reduce_mean_kdims_axis1)(input_emb)\n",
    "        concat_layer = Concatenate(axis=1)(\n",
    "            [mean_layer_op,input_emb,mean_layer_op]\n",
    "        )\n",
    "        n_timesteps = num_domains + 2\n",
    "        bd_lstm = BD_LSTM_layer(concat_layer)\n",
    "\n",
    "        # =========== #\n",
    "        bd_lstm_fwd = bd_lstm[0]\n",
    "        bd_lstm_bck = bd_lstm[1]\n",
    "        # =========== #\n",
    "\n",
    "        split_BL_F_op = Lambda( tf_split_dplus2)(bd_lstm_fwd)\n",
    "        split_BL_B_op = Lambda( tf_split_dplus2)(bd_lstm_bck)\n",
    "        split_BL_B_op = [Lambda( tf_squeeze )(_) for _ in split_BL_B_op]\n",
    "        split_BL_F_op = [Lambda( tf_squeeze )(_) for _ in split_BL_F_op]\n",
    "        print( 'After Bi-Directional LSTM Cur shape :', len(split_BL_B_op), split_BL_B_op[0].shape)\n",
    "\n",
    "        # ----------- #\n",
    "        # Context vector\n",
    "        # ----------- #\n",
    "\n",
    "        ctx_output = []\n",
    "        for i in range(1,n_timesteps-1):\n",
    "            _left = split_BL_F_op[i-1]\n",
    "            _right = split_BL_B_op[i-1]\n",
    "\n",
    "            # Context vector\n",
    "            ctx_concat = Concatenate(axis=-1)([_left,_right])\n",
    "            ctx_mlp_layer1 = list_FNN_1[i-1](ctx_concat)\n",
    "            ctx_mlp_layer2 = list_FNN_2[i-1](ctx_mlp_layer1)\n",
    "            ctx_output.append(ctx_mlp_layer2)\n",
    "\n",
    "        print( ' Cur shape [Context vector]:', ctx_output[0].shape)\n",
    "        # ============\n",
    "        # Final output as sigmoid ( sum of Dot products between context vector and modified input)\n",
    "        # ============\n",
    "\n",
    "        # =--------\n",
    "        # Calculate interaction of the context vector with input  \n",
    "        # =-------- \n",
    "        input_layer_split = Lambda(split_squeeze_numDomains )(input_emb)\n",
    "        interaction_layer_input = [ \n",
    "            xform_Inp_FNN[i](input_layer_split[i]) \n",
    "            for i in range(num_domains) \n",
    "        ]\n",
    "        # Do dot product\n",
    "        dot_product = [ Dot(axes=-1)(\n",
    "            [interaction_layer_input[i],ctx_output[i]]\n",
    "        ) for i in range(num_domains)]\n",
    "\n",
    "        stacked_dot_op = Lambda(tf_stack)(dot_product)\n",
    "\n",
    "        if _type == 'neg':\n",
    "            stacked_dot_op = Lambda(tf_reciprocal)(stacked_dot_op)\n",
    "            final_op = Lambda(tf_reduce_sum)(stacked_dot_op)\n",
    "        else:\n",
    "            final_op = Lambda(tf_reduce_sum)(stacked_dot_op)\n",
    "        final_op = Lambda(tf_sigmoid)(final_op)\n",
    "        return final_op\n",
    "\n",
    "    \n",
    "\n",
    "    if RUN_MODE == 'train':\n",
    "        \n",
    "        # ========= TRAIN mode =========== #\n",
    "        print( 'Run mode ',RUN_MODE )\n",
    "        pos_input = Input(shape=(num_domains,1))\n",
    "        pos_op =  process(pos_input, _type = 'pos')\n",
    "        \n",
    "        neg_input = Input(shape=(num_neg_samples,num_domains,1),name='negative_samples')\n",
    "        neg_input_list = Lambda( tf_split_squeeze_axis1) (neg_input)\n",
    "        neg_ops = []\n",
    "\n",
    "        for n_sample in neg_input_list:\n",
    "            n_res = process(n_sample, _type = 'pos')\n",
    "            neg_ops.append(n_res)\n",
    "\n",
    "        final_pred = Lambda(tf_stack)(neg_ops)\n",
    "        final_pred = Lambda(\n",
    "            lambda x:\n",
    "            tf.math.reduce_mean(x,axis=1,keepdims=False)\n",
    "        )(final_pred)\n",
    "\n",
    "        final_pred = Add()([final_pred, pos_op])\n",
    "        inputs = [pos_input,neg_input]\n",
    "        outputs = final_pred\n",
    "        model = Model(\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "        )\n",
    "        # ====== Fix embedding weights ======= #\n",
    "        for l in model.layers:\n",
    "            if 'entity_embedding_' in l.name:\n",
    "                l.trainable = False\n",
    "    \n",
    "    else:\n",
    "        # ========= TEST mode =========== #\n",
    "        pos_input = Input(shape=(num_domains,1))\n",
    "        print( 'Run mode ',RUN_MODE )\n",
    "        final_pred =  process(pos_input, _type = 'pos')\n",
    "        inputs = pos_input\n",
    "        outputs = final_pred\n",
    "\n",
    "        # ==================================== #\n",
    "        model = Model(\n",
    "                inputs=inputs,\n",
    "                outputs=outputs\n",
    "        )\n",
    "        \n",
    "        model.load_weights(\"model.h5\")\n",
    "        for l in model.layers:\n",
    "            l.trainable = False\n",
    "   \n",
    "    # ============== Custom Loss function ==================== #\n",
    "    # Maximize the objective , Minimize -(predicted_val)\n",
    "    # ======================================================== #\n",
    "    def custom_loss(y_true,y_pred):\n",
    "        return -y_pred\n",
    "    \n",
    "    optimizer = keras.optimizers.Adagrad()\n",
    "    model.compile(optimizer, loss=custom_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# Input : shape [[?, 4, 1],[?,10,4,1]] \n",
    "\n",
    "sizes = [10,20,25,30]\n",
    "pos = []\n",
    "neg = []\n",
    "for i in range(1000):\n",
    "    arr = [np.random.randint(\n",
    "        low=0,\n",
    "        high=j,\n",
    "        size=1\n",
    "    ) for j in sizes]\n",
    "    pos.append(arr)\n",
    "        \n",
    "    n = []\n",
    "    for k in range(10):\n",
    "        arr = [np.random.randint(\n",
    "        low=0,\n",
    "        high=j,size=1\n",
    "        ) for j in sizes]\n",
    "        n.append(arr)\n",
    "   \n",
    "    neg.append(n)\n",
    "\n",
    "pos = np.array(pos)\n",
    "neg = np.array(neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run mode  train\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "num_domains = 4 \n",
    "domain_dims = [10,20,25,30]\n",
    "input_emb_dim = 256\n",
    "lstm_dim = 32\n",
    "ctx_dim_1 = 32\n",
    "interaction_dim = 32\n",
    "num_neg_samples = 10\n",
    "domain_emb_wt = [\n",
    "    np.random.random([domain_dims[i],input_emb_dim])\n",
    "    for i in range(num_domains)\n",
    "]\n",
    "\n",
    "RUN_MODE = 'train'\n",
    "model_obj=get_model(\n",
    "    num_domains=4,\n",
    "    domain_dims=domain_dims,\n",
    "    domain_emb_wt=domain_emb_wt,\n",
    "    lstm_dim=32,\n",
    "    interaction_layer_dim=32,\n",
    "    context_dim=32,\n",
    "    num_neg_samples = 10,\n",
    "    RUN_MODE = 'train'    \n",
    ")\n",
    "\n",
    "save_model(model_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run mode  test\n",
      "After Bi-Directional LSTM Cur shape : 6 (?, 32)\n",
      " Cur shape [Context vector]: (?, 32)\n"
     ]
    }
   ],
   "source": [
    "model_saved = get_model(\n",
    "    num_domains=4,\n",
    "    domain_dims=domain_dims,\n",
    "    domain_emb_wt=domain_emb_wt,\n",
    "    lstm_dim=32,\n",
    "    interaction_layer_dim=32,\n",
    "    context_dim=32,\n",
    "    num_neg_samples = 10,\n",
    "    RUN_MODE = 'test'    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: -2.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe35cd2e6d0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_model(model, inputs, outputs ):\n",
    "def model_train(\n",
    "    model,\n",
    "    pos_x,\n",
    "    neg_x,\n",
    "    batch_size=512,\n",
    "    num_epochs=100\n",
    "):\n",
    "    num_samples = pos_x.shape[0]\n",
    "    y = [None] * num_samples\n",
    "    model.fit(\n",
    "        [pos_x,neg_x],\n",
    "        y,\n",
    "        batch_size = batch_size, \n",
    "        epochs = num_epochs\n",
    "    )\n",
    "    return model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, 4, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3122 (Lambda)            [(None, 1), (None, 1 0           input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "entity_embedding_0 (Embedding)  (None, 1, 256)       2560        lambda_3122[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "entity_embedding_1 (Embedding)  (None, 1, 256)       5120        lambda_3122[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "entity_embedding_2 (Embedding)  (None, 1, 256)       6400        lambda_3122[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "entity_embedding_3 (Embedding)  (None, 1, 256)       7680        lambda_3122[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3123 (Lambda)            (None, 256)          0           entity_embedding_0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3124 (Lambda)            (None, 256)          0           entity_embedding_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3125 (Lambda)            (None, 256)          0           entity_embedding_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3126 (Lambda)            (None, 256)          0           entity_embedding_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3127 (Lambda)            (None, 4, 256)       0           lambda_3123[0][0]                \n",
      "                                                                 lambda_3124[0][0]                \n",
      "                                                                 lambda_3125[0][0]                \n",
      "                                                                 lambda_3126[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3128 (Lambda)            (None, 1, 256)       0           lambda_3127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_626 (Concatenate)   (None, 6, 256)       0           lambda_3128[0][0]                \n",
      "                                                                 lambda_3127[0][0]                \n",
      "                                                                 lambda_3128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional [(None, 6, 32), (Non 73984       concatenate_626[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3129 (Lambda)            [(None, 1, 32), (Non 0           bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3130 (Lambda)            [(None, 1, 32), (Non 0           bidirectional_11[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3137 (Lambda)            (None, 32)           0           lambda_3129[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3131 (Lambda)            (None, 32)           0           lambda_3130[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3138 (Lambda)            (None, 32)           0           lambda_3129[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3132 (Lambda)            (None, 32)           0           lambda_3130[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3139 (Lambda)            (None, 32)           0           lambda_3129[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3133 (Lambda)            (None, 32)           0           lambda_3130[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3140 (Lambda)            (None, 32)           0           lambda_3129[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3134 (Lambda)            (None, 32)           0           lambda_3130[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_627 (Concatenate)   (None, 64)           0           lambda_3137[0][0]                \n",
      "                                                                 lambda_3131[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_628 (Concatenate)   (None, 64)           0           lambda_3138[0][0]                \n",
      "                                                                 lambda_3132[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_629 (Concatenate)   (None, 64)           0           lambda_3139[0][0]                \n",
      "                                                                 lambda_3133[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_630 (Concatenate)   (None, 64)           0           lambda_3140[0][0]                \n",
      "                                                                 lambda_3134[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3143 (Lambda)            [(None, 256), (None, 0           lambda_3127[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_121 (Dense)               (None, 32)           2080        concatenate_627[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_122 (Dense)               (None, 32)           2080        concatenate_628[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_123 (Dense)               (None, 32)           2080        concatenate_629[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_124 (Dense)               (None, 32)           2080        concatenate_630[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_129 (Dense)               (None, 32)           8224        lambda_3143[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_125 (Dense)               (None, 32)           1056        dense_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_130 (Dense)               (None, 32)           8224        lambda_3143[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_126 (Dense)               (None, 32)           1056        dense_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_131 (Dense)               (None, 32)           8224        lambda_3143[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_127 (Dense)               (None, 32)           1056        dense_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_132 (Dense)               (None, 32)           8224        lambda_3143[0][3]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_128 (Dense)               (None, 32)           1056        dense_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_497 (Dot)                   (None, 1)            0           dense_129[0][0]                  \n",
      "                                                                 dense_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_498 (Dot)                   (None, 1)            0           dense_130[0][0]                  \n",
      "                                                                 dense_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_499 (Dot)                   (None, 1)            0           dense_131[0][0]                  \n",
      "                                                                 dense_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_500 (Dot)                   (None, 1)            0           dense_132[0][0]                  \n",
      "                                                                 dense_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3144 (Lambda)            (None, 4, 1)         0           dot_497[0][0]                    \n",
      "                                                                 dot_498[0][0]                    \n",
      "                                                                 dot_499[0][0]                    \n",
      "                                                                 dot_500[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3145 (Lambda)            (None, 1)            0           lambda_3144[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3146 (Lambda)            (None, 1)            0           lambda_3145[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 141,184\n",
      "Trainable params: 0\n",
      "Non-trainable params: 141,184\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_saved.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
